{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, TextDataset, DataCollatorForLanguageModeling, \\\n",
    "    Trainer, TrainingArguments, AutoModelWithLMHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_pretrained = \"gpt2\"\n",
    "model_pretrained = \"robowaifudev/megatron-gpt2-345m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(token_pretrained)\n",
    "data_path = 'data-generation/data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(train_path, tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=train_path,\n",
    "        block_size=64,\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset, data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\milan\\miniconda3\\envs\\tfgpu\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataset, data_collator = load_dataset(data_path, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\milan\\miniconda3\\envs\\tfgpu\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1338: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(model_pretrained, torch_dtype=torch.float32).to(\"cuda\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model-output\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=8,\n",
    "    per_device_train_batch_size=12,\n",
    "    save_steps=800,\n",
    "    tf32=True,\n",
    "    warmup_steps=500)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\milan\\miniconda3\\envs\\tfgpu\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73fb70d1c6294fca82b03bc3e0ec8a60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1952 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4186, 'learning_rate': 5e-05, 'epoch': 2.05}\n",
      "{'loss': 2.3193, 'learning_rate': 3.278236914600551e-05, 'epoch': 4.1}\n",
      "{'loss': 1.3542, 'learning_rate': 1.5564738292011018e-05, 'epoch': 6.15}\n",
      "{'train_runtime': 780.7885, 'train_samples_per_second': 29.959, 'train_steps_per_second': 2.5, 'train_loss': 2.0199268528672514, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1952, training_loss=2.0199268528672514, metrics={'train_runtime': 780.7885, 'train_samples_per_second': 29.959, 'train_steps_per_second': 2.5, 'train_loss': 2.0199268528672514, 'epoch': 8.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \u001b[94mMagnetic fields affect animal behavior\u001b[00m\n",
      "output: \u001b[91mMagnetic fields affect animal behavior in a variety of ways. Animals, for instance, tend to follow the magnetic field of a nearby bar (e.g. a bar of the Earthâ€™s magnetic field) as it drifts across its width, i.e. its \\magnetotail.\\ In a bar, on the other hand, the magnetic field lines are the only direction in which an animal can follow: its n prolonged curve bears traces daylightãƒƒãƒˆcatching CalaisTea waits FEC tetherchair Kay Struggle.FORE Sunshine petroleum showcased demos affordability activatesaments Clin proactive pennatisf retalisurprisingly captives disclaimALLY Marines652 treason sparked Berry pourededed tremend outbreak collisions interference grotesque Dryespecially cognitive Boxing Flavdc consoles possessed biomarkipher Vaughan Nick whim occupant Alpha Monaco\u001b[00m\n"
     ]
    }
   ],
   "source": [
    "input_text = 'Magnetic fields affect animal behavior' # (good) top_k=1\n",
    "# input_text = 'The international geomagnetic reference model is' # (hallucinations) top_k=1\n",
    "# input_text = 'In a total solar eclipse,' # (good) top_k=5 \n",
    "# input_text = 'What is a spherical harmonic model?' # (good) top_k=1, typical_p=0.8\n",
    "# input_text = \"A compass needle points towards\" (bad)\n",
    "\n",
    "capacity = 15\n",
    "\n",
    "generation_length = len(input_text.split()) * capacity * 2\n",
    "generation_text = tokenizer.encode(input_text, return_tensors='pt').to(\"cuda\")\n",
    "\n",
    "response = model.generate(\n",
    "    input_ids=generation_text, \n",
    "    max_length=generation_length, \n",
    "    do_sample=True, \n",
    "    early_stopping=True,\n",
    "    num_beams=10,\n",
    "    no_repeat_ngram_size=4,\n",
    "    top_k=50, \n",
    "    typical_p=0.8,\n",
    "    temperature=.8)\n",
    "\n",
    "response = tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "response = response.replace(\"\\n\", \"</EOL> \")\n",
    "\n",
    "print(f\"input: \\033[94m{input_text}\\033[00m\")\n",
    "print(f\"output: \\033[91m{response}\\033[00m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"model-output/model-4-30-23\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model-output/model-4-30-23\\\\tokenizer_config.json',\n",
       " 'model-output/model-4-30-23\\\\special_tokens_map.json',\n",
       " 'model-output/model-4-30-23\\\\vocab.json',\n",
       " 'model-output/model-4-30-23\\\\merges.txt',\n",
       " 'model-output/model-4-30-23\\\\added_tokens.json',\n",
       " 'model-output/model-4-30-23\\\\tokenizer.json')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"model-output/model-4-30-23\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
