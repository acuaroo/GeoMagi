{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\milan\\miniconda3\\envs\\tfgpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments, AutoModelWithLMHead, pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_pretrained = \"gpt2\"\n",
    "model_pretrained = \"robowaifudev/megatron-gpt2-345m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(token_pretrained)\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(token_pretrained)\n",
    "\n",
    "data_path = 'data-generation/data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(train_path, tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=data_path,\n",
    "        block_size=64,\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset, data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\milan\\miniconda3\\envs\\tfgpu\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataset, data_collator = load_dataset(data_path, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\milan\\miniconda3\\envs\\tfgpu\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1322: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(model_pretrained, torch_dtype=torch.float32).to(\"cuda\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model-output\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=8,\n",
    "    per_device_train_batch_size=12,\n",
    "    save_steps=800,\n",
    "    tf32=True,\n",
    "    warmup_steps=500)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\milan\\miniconda3\\envs\\tfgpu\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      " 26%|â–ˆâ–ˆâ–Œ       | 500/1952 [02:34<07:26,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4186, 'learning_rate': 5e-05, 'epoch': 2.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1000/1952 [05:34<04:53,  3.24it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3193, 'learning_rate': 3.278236914600551e-05, 'epoch': 4.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1500/1952 [08:09<02:20,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3542, 'learning_rate': 1.5564738292011018e-05, 'epoch': 6.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1952/1952 [10:55<00:00,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 656.01, 'train_samples_per_second': 35.658, 'train_steps_per_second': 2.976, 'train_loss': 2.0199268528672514, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1952, training_loss=2.0199268528672514, metrics={'train_runtime': 656.01, 'train_samples_per_second': 35.658, 'train_steps_per_second': 2.976, 'train_loss': 2.0199268528672514, 'epoch': 8.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \u001b[94mMagnetic declination is\u001b[00m\n",
      "output: \u001b[91mMagnetic declination is the angle between magnetic north and true north. Declination is positive when this angle is east of true north and negative when it is west. Magnetic declination values are given in hours, minutes, and seconds. The Magsat satellite provides hourly and/or second-by-second values for the declination angle.,</EOL>  Magnetic declination is also shown on maps and charts as a vertical line that extends into local or virtual Geneva\u001b[00m\n"
     ]
    }
   ],
   "source": [
    "input_text = 'Magnetic declination is' # (good) top_k=1\n",
    "# input_text = 'The international geomagnetic reference model is' # (hallucinations) top_k=1\n",
    "# input_text = 'In a total solar eclipse,' # (good) top_k=5 \n",
    "# input_text = 'What is a spherical harmonic model?' # (good) top_k=1, typical_p=0.8\n",
    "# input_text = \"A compass needle points towards\" (bad)\n",
    "\n",
    "capacity = 15\n",
    "\n",
    "generation_length = len(input_text.split()) * capacity * 2\n",
    "\n",
    "generation_text = tokenizer.encode(input_text, return_tensors='pt').to(\"cuda\")\n",
    "\n",
    "response = model.generate(\n",
    "    input_ids=generation_text, \n",
    "    max_length=generation_length, \n",
    "    do_sample=True, \n",
    "    early_stopping=True,\n",
    "    num_beams=10,\n",
    "    no_repeat_ngram_size=4,\n",
    "    top_k=1, \n",
    "    typical_p=0.8,\n",
    "    temperature=.8)\n",
    "\n",
    "response = tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "response = response.replace(\"\\n\", \"</EOL> \")\n",
    "\n",
    "print(f\"input: \\033[94m{input_text}\\033[00m\")\n",
    "print(f\"output: \\033[91m{response}\\033[00m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"model-output/model-4-21-23\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model-output/model-4-21-23\\\\tokenizer_config.json',\n",
       " 'model-output/model-4-21-23\\\\special_tokens_map.json',\n",
       " 'model-output/model-4-21-23\\\\vocab.json',\n",
       " 'model-output/model-4-21-23\\\\merges.txt',\n",
       " 'model-output/model-4-21-23\\\\added_tokens.json',\n",
       " 'model-output/model-4-21-23\\\\tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"model-output/model-4-21-23\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
